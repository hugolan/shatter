[DATASET]
dataset_package = decentralizepy.datasets.CIFAR10
dataset_class = CIFAR10
model_class = LeNet
; provide directory containing "cifar-10-batches-py" folder | Pre-download recommended
; New download does not work with multiple processes | Crashes the first time, just retry
train_dir = ../../eval/data/
test_dir = ../../eval/data/
; python list of fractions below
sizes = 
random_seed = 7,33,1,44,34,1006,78000
;90,42,123,789,1001,5555,8675309,314159,7,33,1,44,34,1006,78000
partition_niid = dirichlet
alpha = 0.5
; alpha (dirichlet parameter)

[OPTIMIZER_PARAMS]
optimizer_package = torch.optim
optimizer_class = SGD
lr = 0.05

; gamma

[TRAIN_PARAMS]
training_package = decentralizepy.training.Training
training_class = Training
rounds = 10
; r
full_epochs = False
batch_size = 5
; b
shuffle = True
loss_package = torch.nn
loss_class = CrossEntropyLoss

[COMMUNICATION]
comm_package = decentralizepy.communication.TCP
comm_class = TCP
addresses_filepath = /home/hugo/shatter/decentralizepy/tutorial/EpidemicLearning/ip.json

[SHARING]
sharing_package = decentralizepy.sharing.PlainAverageSharing
; Does not use Metropolis-Hastings
sharing_class = PlainAverageSharing
compress = False

[NODE]
graph_degree = 5
; s (number of neighbors in EL-Oracle and number of random neighbors picked to send message to in EL-Local)