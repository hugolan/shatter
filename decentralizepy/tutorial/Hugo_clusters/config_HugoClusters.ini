[DATASET]
dataset_package = decentralizepy.datasets.CIFAR10
dataset_class = CIFAR10
model_class = LeNet
; provide directory containing "cifar-10-batches-py" folder | Pre-download recommended
; New download does not work with multiple processes | Crashes the first time, just retry
train_dir = ../../eval/data/
test_dir = ../../eval/data/
; python list of fractions below
sizes = 
random_seed = 90,42,123,2024
;90
;42,123,2024,789,1001,5555,8675309,314159,888
;90
partition_niid = dirichlet_clusters
clusters = 4
alpha_1 = 0.3, 0.4
alpha_2 = 0.7, 0.8
; alpha (dirichlet parameter)

[OPTIMIZER_PARAMS]
optimizer_package = torch.optim
optimizer_class = SGD
lr = 0.05
;0.1,0.025,0.0125
;weight_decay = 0.01
;,0.0125,0.1
; gamma

[TRAIN_PARAMS]
training_package = decentralizepy.training.TrainingNIID
training_class = Training
rounds = 10
; r
full_epochs = False
batch_size = 5
; b
shuffle = True
loss_package = torch.nn
loss_class = CrossEntropyLoss

[COMMUNICATION]
comm_package = decentralizepy.communication.TCP
comm_class = TCP
addresses_filepath = /home/hugo/shatter/decentralizepy/tutorial/Hugo_clusters/ip.json

[SHARING]
sharing_package = decentralizepy.sharing.PlainAverageSharing
; Does not use Metropolis-Hastings
sharing_class = PlainAverageSharing
compress = False

[NODE]
graph_degree = 3
; s (number of neighbors in Hugo-Oracle and number of random neighbors picked to send message to in Hugo-Local)

[PARAMS]
el_start = 0
distance_nodes = 2
distance_similarity = furtherexp
weighting_factor = 10
;,closer
alternate_rounds = 0
;3,4,5,6,8,10,20,100
;,closer